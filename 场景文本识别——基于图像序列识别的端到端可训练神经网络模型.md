# 场景文本识别——基于图像序列识别的端到端可训练神经网络模型

<a name="idjwv"></a>
# 1. 背景介绍
<a name="KoovS"></a>
## 1.1 主题介绍
由于神经网络的强大复兴，特别是深度卷积神经网络（DCNN）模型在各种视觉任务中的巨大成功的推动，最近大多数与深度神经网络相关的工作主要致力于检测或分类对象类别。基于图像的序列识别问题一直是计算机视觉中长期存在的研究课题。本文将介绍由 Baoguang Shi 等人发表的一篇论文 [_An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition_](https://arxiv.org/abs/1507.05717)。在该论文中，作者研究了**场景文本识别**的问题，这是**基于图像的序列识别**中最重要和最具有挑战性的任务之一。相较于一般的对象识别任务，基于图像序列识别任务中的识别对象往往是以序列的形式出现，而不是孤立地出现。因此，识别这样的对象通常需要系统预测一系列标签而不是单个标签。序列对象另一个独特之处在于它们的长度可能发生较大的变化。
<a name="cuR43"></a>
## 1.2 研究背景
对于特定的序列对象（例如场景文本），人们已经做过一些尝试来解决此类对象的识别问题。例如，利用某种算法检测出单个字符，然后利用训练好的 **DCNN 模型**来识别检测到的单个字符。此类方法通常需要训练强大的字符检测器，以便从原始单词图像中精确地检测和裁剪每个字符。另外，一些其他方法将场景文本识别视为图像分类问题，为每个英文单词（总共 9 万个单词）分配一个类标签。运用此类方法产生的模型将有众多的类标签，因此很难泛化到其他类型的序列对象。例如，中文文本、音乐配乐等序列对象拥有上百万种类标签，该模型很难泛化到此类序列对象。因此，基于 DCNN 的系统不能直接用于基于图像的序列识别。**循环神经网络（RNN）模型**是深度神经网络家族中另一个重要分支，用于处理序列问题。RNN 的一大优势是在训练和测试的时候，不需要序列对象图像中每一个元素的位置信息。然而，将输入图像转换成图像特征序列的预处理步骤是必须的。预处理步骤独立于 RNN 模型的流程步骤，因此基于RNN的现有系统不能以端到端的方式进行训练和优化。<br />除了深度神经网络之外，一些**传统的场景文本识别方法**也为这一领域带来了一些新颖的想法，并且取得了优异的表现。例如，将单词图像和文本字符串嵌入到公共向量子空间并将词识别转化为检索问题、利用中层特征进行场景文本识别等方法在标准数据集上取得了有效的性能。但是此类方法与基于神经网络的算法相比较而言，模型的表现还是较差。<br />在这篇文章中，作者提出了一种将特征提取、序列建模和转录整合到统一框架中的新型神经网络模型——专门用于基于图像的图像序列的识别。这种神经网络模型被称为**卷积循环神经网络（CRNN）模型**，因为它是 DCNN 模型和 RNN 模型的组合。
<a name="LOf48"></a>
# 2. 网络结构
如图 1 所示，CRNN模型 的网络架构由三部分组成，包括卷积层、循环层和转录层（由下到上）。在 CRNN 的底部，**卷积层**自动从每个输入图像中提取特征序列。在卷积网络层之上，构建一个对卷积层特征序列的每一帧进行预测的**循环网络层**。在 CRNN 顶部的**转录层**将循环层输出的帧预测转化为标签序列。CRNN 模型虽然由不同类型的网络架构组成，但可以通过一个损失函数进行联合训练。<br />![场景文本识别1.png](https://cdn.nlark.com/yuque/0/2019/png/391201/1565104619501-3e5e6c55-b634-412c-85ee-d44afde114bb.png#align=left&display=inline&height=535&name=%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB1.png&originHeight=535&originWidth=483&size=96745&status=done&width=483)<br />**图 1. 网络架构**<br />架构包括三个部分：1）卷积层，从输入图像中提取特征序列；2）循环层，预测每一帧的标签分布；3）转录层，将每一帧的预测变为最终的标签序列。
<a name="UcXf5"></a>
## 2.1 特征序列提取
在 CRNN 模型中，卷积层部分由标准 CNN 模型中的卷积层和最大池化层组成。这一部分用来提取表示输入图片的序列特征。在输入到网络之前，所有的的图片需要被缩放到同一高度。然后，一些列的特征向量从特征图谱中被提取出来。这些特征图谱由卷积层产生，作为循环层的输入。值得注意的是，每个特征序列的特征向量在特征图谱中从左到右一列一列地产生。这意味着第 i 个特征向量是所有特征图谱第 i 列向量的联合体。在该论文中，每列的宽度被固定设置为单个像素。<br />卷积、最大池化和激活函数作用在局部区域，具有平移不变性。因此，特征图谱的每一列与原始图片中的一块矩形区域对应，我们称之为感受野。这些矩形区域从左到右和特征图谱中的列向量一一对应。如图 2 所示，特征序列中的每一个向量与对应的感受野相关联，被看做是相应区域的图像描述符。<br />![场景文本识别2.jpg](https://cdn.nlark.com/yuque/0/2019/jpeg/391201/1565143523452-a8f8d786-1b17-4683-a69a-5c1e36351b40.jpeg#align=left&display=inline&height=251&name=%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB2.jpg&originHeight=181&originWidth=234&size=11330&status=done&width=325)<br />**图 2. 局部感受野**<br />提取出的特征序列中的每一个向量与输入图像中的感受野相关联，它们可以被认为是对应区域的特征向量。<br />由于具有较高的鲁棒性和良好的可训练性，深层卷积特征已经被广泛运用于各种视觉识别任务。一些以前的方法都利用 CNN 来学习一个具有较好鲁棒性的表示。然而，这些方法通常通过 CNN 提取整个图像的全部表示，然后收集局部深度特征来识别序列对象的每个分量。由于 CNN 要求将输入图像缩放到固定尺寸以满足其固定的输入尺寸，所以它不适合序列对象，而且它们长度变化较大。在 CRNN 中，我们将深度特征传递到序列表示中，以便对序列对象的长度变化保持不变。
<a name="2lQbo"></a>
## 2.2 序列标记
在卷积层之上是一个**深度双向循环神经网络**来作为循环层。循环层为特征序列的每一帧预测一个标签分布。循环层的优点有三重。首先，RNN 具有很强的捕获序列上下文信息的能力。由于使用了上下文信息，基于图像的序列识别变得更加稳定有效，相比于仅仅单独使用每个标志而言。以场景文本识别为例，较宽的字符可能需要一些连续的帧来描述。此外，在观察完上下文信息之后，一些模糊的字符会更容易区分。例如，我们在识别 "il" 的时候，通过对比两个字母的高度，我们可以更加容易地识别出它们。其次，RNN 可以将误差值反向传播到其输入层，即卷积层。从而允许我们在统一的网络中共同训练循环层和卷积层。最后，RNN 能够从头到尾对任意长度的序列进行操作。<br />![场景文本识别3.jpg](https://cdn.nlark.com/yuque/0/2019/jpeg/391201/1565147721271-4b0c5598-89e6-42d5-b583-699a9f2a549d.jpeg#align=left&display=inline&height=346&name=%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB3.jpg&originHeight=346&originWidth=618&size=40176&status=done&width=618)<br />**图 3. **（a）基本的 LSTM 单元结构。LSTM 包括单元模块和三个门，即输入门、输出门和遗忘门；（b）论文中使用的深度双向 LSTM 结构。合并前向和后向 LSTM 的结果到双向 LSTM中。堆叠多个双向 LSTM 产生一个深度双向 LSTM 结构。<br />传统 RNN 单元在其输入和输出层之间具有自动连接的隐藏层。每次接收到序列中的帧 ![](https://cdn.nlark.com/yuque/__latex/db1588c57f55d31c1e7cd6db73d7d7ea.svg#card=math&code=x%5Et&height=24&width=15) 时，它将使用非线性函数来更新其内部状态 ![](https://cdn.nlark.com/yuque/__latex/615562964fd713a491d21b4ff3b801db.svg#card=math&code=h%5Et&height=24&width=15) ，该非线性函数同时接收当前输入 ![](https://cdn.nlark.com/yuque/__latex/db1588c57f55d31c1e7cd6db73d7d7ea.svg#card=math&code=x%5Et&height=24&width=15) 和过去状态 ![](https://cdn.nlark.com/yuque/__latex/6f9194a0062e088087e65ba06a358771.svg#card=math&code=h%5E%7Bt-1%7D&height=24&width=30) 作为其输入：![](https://cdn.nlark.com/yuque/__latex/9318ab80822036863a7f8882528bbaf0.svg#card=math&code=h_t%3Dg%28x_t%2Ch_%7Bt-1%7D%29&height=24&width=110)。<br />那么预测是基于 ![](https://cdn.nlark.com/yuque/__latex/615562964fd713a491d21b4ff3b801db.svg#card=math&code=h%5Et&height=24&width=15) 的。以这种方式，过去的上下文 ![](https://cdn.nlark.com/yuque/__latex/ad815d40738188a7c657f27c0322adfe.svg#card=math&code=x_%7Bt%27%7D&height=24&width=19) 被捕获并用于预测。然而，传统的 RNN 单元有梯度消失的问题，这限制了其可存储的上下文范围，并且给训练过程增加了负担。[长短期记忆网络（LSTM）](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)是一种专门设计用于解决**梯度消失**问题的 RNN单元。LSTM 由一个存储单元和三个门组成，即输入门、输出门和遗忘门。在概念上，存储单元存储过去的上下文，并且输入门和输出门允许其长时间存储。同时，存储单元中的存储内容可以被遗忘门清除。LSTM 的特殊设计允许它捕获长距离依赖，这经常发生在基于图像的序列中。<br />LSTM 是定向的，它只使用过去的上下文。在基于图像的序列中，两个方向的上下文是互补的。因此，我们将两个不同方向的 LSTM 组合到一个**双向 LSTM **中。此外，可以将多个双向 LSTM 堆叠起来，得到图 3.b 所示的双向 LSTM。深层结构允许相比浅层结构能够得到一个更高层次的抽象，并且在语音识别任务中取得显著的性能改进。<br />在循环层中，误差在图 3.b 所示箭头的相反方向传播，即**基于时间的反向传播（BPTT）**。在循环层的底部，传播的微分序列被连接成图谱，反转了将特征图谱转换为特征序列的操作，并且被反馈至卷积层。实际上，我们创建了一个称为“**Map-to-Sequence**”的自定义网络层，作为卷积层和循环层之间的桥梁。
<a name="oo9VN"></a>
## 2.3 转录
转录是将 RNN 所做的每帧预测转换成标签序列的过程。数学上，转录是根据每帧预测找到具有最高概率的标签序列。在实践中，存在两种转录模式，即无词典转录和基于词典的转录。词典是一组标签序列，预测时会受到拼写检查字典约束。在无词典模式中，预测时没有任何词典。在基于词典的模式中，通过选择具有最高概率的标签序列进行预测。
<a name="Ycc20"></a>
### 2.3.1 标签序列的概率
我们采用 Graves 等人提出的**联接时间分类（**[**CTC**](https://blog.csdn.net/Left_Think/article/details/76370453)**）**层中的条件概率。标签序列 ![](https://cdn.nlark.com/yuque/__latex/2db95e8e1a9267b7a1188556b2013b33.svg#card=math&code=l&height=24&width=5) 的概率由每一帧的预测结构决定 ![](https://cdn.nlark.com/yuque/__latex/539321986de009a31403c5b33e0070e4.svg#card=math&code=y%20%3D%20y_1%2C...%2Cy_T&height=24&width=99)，并且忽略 ![](https://cdn.nlark.com/yuque/__latex/2db95e8e1a9267b7a1188556b2013b33.svg#card=math&code=l&height=24&width=5) ** **中每个标签所在的位置。因此，当我们使用这种概率的负对数作为训练网络的目标函数的时候，我们只需要图像及其相应的标签序列，避免了标注单个字符位置的劳动。<br />标签序列的条件概率公式如下：<br />![](https://cdn.nlark.com/yuque/__latex/9f12a3056b985ec43712a5fd77bd028e.svg#card=math&code=p%28l%7Cy%29%20%3D%20%5Csum_%7B%5Cpi%3AB%28%5Cpi%29%3D1%7Dp%28%5Cpi%7Cy%29&height=42&width=154)，                                                                     （1）<br />其中 ![](https://cdn.nlark.com/yuque/__latex/415290769594460e2e485922904f345d.svg#card=math&code=y&height=24&width=8) 是输入序列，![](https://cdn.nlark.com/yuque/__latex/b9ece18c950afbfa6b0fdbfa4ff731d3.svg#card=math&code=T&height=24&width=11) 是序列长度，![](https://cdn.nlark.com/yuque/__latex/9d5ed678fe57bcca610140957afab571.svg#card=math&code=B&height=24&width=12) 表示将序列 ![](https://cdn.nlark.com/yuque/__latex/4f08e3dba63dc6d40b22952c7a9dac6d.svg#card=math&code=%5Cpi&height=24&width=9) 映射为 ![](https://cdn.nlark.com/yuque/__latex/2db95e8e1a9267b7a1188556b2013b33.svg#card=math&code=l&height=24&width=5) 的映射；<br />![](https://cdn.nlark.com/yuque/__latex/4f08e3dba63dc6d40b22952c7a9dac6d.svg#card=math&code=%5Cpi&height=24&width=9) 的概率定义为 ![](https://cdn.nlark.com/yuque/__latex/54b9e4141b204ba1ca10a370d4b6b4a1.svg#card=math&code=p%28%5Cpi%7Cy%29%3D%5Cprod_%7Bt%3D1%7D%5ET%20y_%7B%5Cpi_t%7D%5Et&height=51&width=109) ，![](https://cdn.nlark.com/yuque/__latex/daf7b9c2c1cb1488d09365f610b12828.svg#card=math&code=y_%7B%5Cpi_t%7D%5Et&height=24&width=21) 是在时间戳 ![](https://cdn.nlark.com/yuque/__latex/e358efa489f58062f10dd7316b65649e.svg#card=math&code=t&height=24&width=6) 时有标签 ![](https://cdn.nlark.com/yuque/__latex/72c88d9295c4086717f63756ec75c06a.svg#card=math&code=%5Cpi_t&height=24&width=15) 的概率。
<a name="4jJch"></a>
### 2.3.2 无词典转录
在无词典转录的模式下，序列 ![](https://cdn.nlark.com/yuque/__latex/2db95e8e1a9267b7a1188556b2013b33.svg#card=math&code=l&height=24&width=5) 出现的概率作为预测，正如式（1）所示。由于找不到精确解的可行方法，作者在论文中使用**最大似然估计**的方法来确定概率最大的标签序列。序列 ![](https://cdn.nlark.com/yuque/__latex/2db95e8e1a9267b7a1188556b2013b33.svg#card=math&code=l&height=24&width=5) 通过 ![](https://cdn.nlark.com/yuque/__latex/8fefdfac0756163e1769be3d87653461.svg#card=math&code=l%5E%2A%20%5Capprox%20B%28arg%20max%20_%7B%5Cpi%7Dp%28%5Cpi%7Cy%29%29&height=24&width=166) 近似发现，即在每个时间戳 ![](https://cdn.nlark.com/yuque/__latex/e358efa489f58062f10dd7316b65649e.svg#card=math&code=t&height=24&width=6) 采用最大概率的标签 ![](https://cdn.nlark.com/yuque/__latex/72c88d9295c4086717f63756ec75c06a.svg#card=math&code=%5Cpi_t&height=24&width=15) ，并将结果序列映射到 ![](https://cdn.nlark.com/yuque/__latex/2db95e8e1a9267b7a1188556b2013b33.svg#card=math&code=l&height=24&width=5) 。
<a name="PMeHY"></a>
### 2.3.3 基于词典的转录
在基于词典的模式中，每个测试采样与词典相关联。基本上，标签序列被识别为由等式（1）所定义的概率最大的标签序列。概率最大时，对应的标签序列为 ![](https://cdn.nlark.com/yuque/__latex/ba8c9a159912e257787d8651dc9d394c.svg#card=math&code=l%5E%2A%3Dargmax_%7Bl%5Cin%20D%7Dp%28l%7Cy%29&height=24&width=150)  (2) 。<br />然而，对于大型词进行搜索并选择概率最高的一个序列是非常耗时的。为了解决这个问题，我们发现通过无词典转录预测的标签序列在编辑距离度量下是非常接近真实结果的。因此，我们可以在由无词典转录预测的标签序列的最大编辑距离的范围内搜索最近邻近候选目标。<br />通过** BK 树**数据结构，我们可以有效地找到候选目标，这是一种适用于离散度量空间的度量树。BK 树的搜索时间复杂度为 ![](https://cdn.nlark.com/yuque/__latex/09a63fc19421d0b297eb7f5a2f9b1084.svg#card=math&code=O%28log%7CD%7C%29&height=24&width=68)，其中 |D| 词典大小。在该论文中，作者为每一个离线词典构造一个 BK 树。然后，使用 BK  树执行快速在线搜索来找到小于或等于编辑距离 ![](https://cdn.nlark.com/yuque/__latex/77a3b715842b45e440a5bee15357ad29.svg#card=math&code=%5Cdelta&height=24&width=7) 的序列。
<a name="ffsFJ"></a>
## 2.4 网络训练 
数据集 ![](https://cdn.nlark.com/yuque/__latex/f4db3b8266ba512f8f31f11531558bcb.svg#card=math&code=%5Cchi%20%3D&height=24&width=27) ![](https://cdn.nlark.com/yuque/__latex/3c924626dfa5a27a1d72d6ddfda8854a.svg#card=math&code=%5C%7BI_i%2Cl_i%5C%7D_i&height=24&width=52) 表示训练集，![](https://cdn.nlark.com/yuque/__latex/ff3e5d497ea81bedd8ea917f06223313.svg#card=math&code=I_i&height=24&width=13) 是训练图像，![](https://cdn.nlark.com/yuque/__latex/b39335b6584e8455ab4de3c86b439e21.svg#card=math&code=l_i&height=24&width=10) 是真实的标签序列。网络训练的目标就是最小化真实条件概率的负似然对数：      ![](https://cdn.nlark.com/yuque/__latex/d1904f0459b87a8414f836465b3625ad.svg#card=math&code=O%3D-%5Csum_%7BI_i%2Cl_i%5Cin%5Cchi%7D%20logp%28l_i%7Cy_i%29&height=41&width=157)    （3）。![](https://cdn.nlark.com/yuque/__latex/8d62e469fb30ed435a668eb5c035b1f6.svg#card=math&code=y_i&height=24&width=14) 是循环层和卷积层从 ![](https://cdn.nlark.com/yuque/__latex/ff3e5d497ea81bedd8ea917f06223313.svg#card=math&code=I_i&height=24&width=13) 生成的序列，目标函数直接从图像和真实标签序列计算代价函数。因此，网络可以在成对的图像和序列上进行**端对端**训练，去除了在训练图像中手动标记所有单独组件的过程。<br />网络使用随机梯度下降（SGD）进行训练，梯度由反向传播算法进行计算。为了优化，我们使用 [**ADADELTA**](https://arxiv.org/abs/1212.5701)** **自动计算每一维的学习率。与传统的动量方法相比，ADADELTA 不需要手动设置学习率。更重要的是， ADADELTA的优化收敛速度比动量方法更快。
<a name="IuY5y"></a>
# 3. 实验
为了评估 CRNN 模型的**有效性**，作者在场景文本识别和乐谱识别的标准基数据集上进行了实验。数据集和训练测试的设置见 3.1 小节，场景文本图像中 CRNN 的详细设置见 3.2 小节，综合比较的结果在 3.3 小节报告。为了进一步验证该模型的**泛化性**，在 3.4 小节中作者用乐谱识别任务作出了验证。
<a name="SCTpm"></a>
## 3.1 数据集
在本论文中，作者使用 Jaderberg 等人发布的[合成数据集](https://www.robots.ox.ac.uk/~vgg/publications/2014/Jaderberg14c/)作为训练数据。数据集包含八百万训练图像及其对应的实际单词。使用四个流行的基准数据集作为模型的性能评估，即 [ICDAR 2003](http://www.iapr-tc11.org/mediawiki/index.php?title=ICDAR_2003_Robust_Reading_Competitions)（IC03）、[ICDAR 2013]()（IC13）、[IIIT 5k-word](http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/IIIT5K.html) （IIIT5k）和 [Street View Text](http://www.iapr-tc11.org/mediawiki/index.php?title=The_Street_View_Text_Dataset)（SVT）。CRNN 模型虽然是在纯合成文本数据上进行训练的，但是它在标准文本识别数据集上的表现良好。
<a name="3GGJl"></a>
## 3.2 实现细节
在实验中，作者使用的网络配置总结在表 1 中，卷积层的架构是基于 [VGG](https://arxiv.org/abs/1409.1556) 的架构。为了使其适用于识别英文文本，对相应网络层进行了调整：在第 3 和第 4 个最大池化层中，采用的是 1*2 的矩形池化窗口。这种调整产生宽度较大的特征图，因此具有更长的特征序列。最重要的是，矩形池窗口产生矩形感受野（如图 2 所示），这有助于识别一些具有窄形状的字符。<br />网络不仅有深度卷积层，而且还有循环层。两种网络结构都难以训练，作者在第 5 和 第 6 个卷积层之后插入两个批归一化层，可以大大加快网络的训练速度。<br />![场景文本识别4.png](https://cdn.nlark.com/yuque/0/2019/png/391201/1565523017654-7818f420-8421-497a-a1e5-b30325b1109e.png#align=left&display=inline&height=421&name=%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB4.png&originHeight=421&originWidth=414&size=84485&status=done&width=414)<br />**表 1. **网络配置结构<br />

<a name="5Lw6t"></a>
## 3.3 比较评估
提出的 CRNN 模型及其他一些算法在上述四个公共数据集上获得的所有识别精度如表 2 所示。在有约束词典的情况中，该模型的方法始终优于大多数最新的方法。并且 CRNN 不限于识别已知词典中的单词，并且能够处理随机字符串、句子或其他诸如中文单词的脚本。因此，CRNN 的结果在所有的测试集上都具有竞争力。在无约束词典的情况下，该模型在 SVT 上取得了最佳性能，但在 IC03 和 IC13 上仍落后于一些方法。表 2 中，"None" 列空白表示该方法不能用于无约束词典的情况。<br />![场景文本识别5.png](https://cdn.nlark.com/yuque/0/2019/png/391201/1565523899074-6fb53446-1de6-4a0c-9ca8-cbdfccb173e9.png#align=left&display=inline&height=440&name=%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB5.png&originHeight=440&originWidth=887&size=83957&status=done&width=887)<br />**表 2. **准确率评估
<a name="WxT5y"></a>
## 3.4 乐谱识别
乐谱通常由排列在五线谱的音符序列组成。识别图像中的乐谱被称为光学音乐识别（OMR）问题。以前的方法通常需要图像预处理（主要是二值化），五线谱检测和单个音符识别。作者将 OMR 作为序列识别问题，直接用CRNN 从图像中预测音符的序列。<br />为了准备 CRNN 所需的训练数据，作者收集了 2650 张图像。每个图像中有一个包含 3 到 20 个音符的乐谱片段。并且手动标记所有图像的真实标签序列。收集到的图像通过旋转，缩放和用噪声损坏增强到了265k个训练样本，并用自然图像替换它们的背景。对于测试，作者创建了图4 所示的三个数据集。<br />![场景文本识别6.jpg](https://cdn.nlark.com/yuque/0/2019/jpeg/391201/1565524318680-d02bdb10-24b2-4878-bcdf-16eb0ff18f05.jpeg#align=left&display=inline&height=410&name=%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB6.jpg&originHeight=410&originWidth=497&size=59985&status=done&width=497)<br />**图4.** a) 收集到的干净乐谱图像；b) 合成的乐谱图像；c) 手机拍摄的现实世界中的乐谱图像<br />实验表明，CRNN ，模型优于两个商业系统——Capella Scan 和 PhotoScore 系统。另一方面，CRNN 模型使用对噪声和扭曲具有鲁棒性的**卷积特征**。此外，CRNN 中的**循环层**可以利用乐谱中的上下文信息。每个音符不仅自身被识别，而且被附近的音符识别。因此，通过将一些音符与附近的音符进行比较从而可以识别它们。<br />![场景文本识别7.jpg](https://cdn.nlark.com/yuque/0/2019/jpeg/391201/1565525102497-97d2d377-ac72-4e4f-943c-ce92d85eebc9.jpeg#align=left&display=inline&height=92&name=%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB7.jpg&originHeight=92&originWidth=390&size=14714&status=done&width=390)<br />**表3.** OMR 实验比较<br />实验结果显示了 CRNN 的泛化性，因为它可以很容易地应用于其他的基于图像的序列识别问题，需要极少的领域知识。它为 OMR 提供了一个新的方案，并且在音高识别方面表现出极大的潜力。
<a name="0g2Gy"></a>
# 4. 总结
在本文中，我们提出了一种新颖的神经网络架构，称为卷积循环神经网络（CRNN），其集成了卷积神经网络（CNN）和循环神经网络（RNN）的优点。CRNN 能够获取不同尺寸的输入图像，并产生不同长度的预测。它直接在粗粒度的标签（例如单词）上运行，在训练阶段不需要详细标注每一个单独的元素（例如字符）。此外，由于CRNN放弃了传统神经网络中使用的全连接层，因此得到了更加紧凑和高效的模型。此外，CRNN 在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了 CRNN 的泛化性。<br />实际上，CRNN 是一个通用框架，因此可以应用于其它的涉及图像序列预测的领域和问题（如汉字识别）。进一步加快CRNN，使其在现实应用中更加实用，是未来值得探索的另一个方向。该模型在 [Mo 平台](https://momodel.cn/)上有实现，大家可以搜索[场景文本识别](https://momodel.cn/workspace/5d4bec151afd943022191a50?type=app)找到。<br />**项目源码地址：**[**https://momodel.cn/explore/5d5b51b630d61891c425852b?type=app**](https://momodel.cn/explore/5d5b51b630d61891c425852b?type=app)
<a name="GB5Xr"></a>
# 5. 参考资料

1. 论文： [An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition](https://arxiv.org/abs/1507.05717)
1. 论文：[Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
1. 博客：[基于图像序列识别的端到端可训练神经网络模型](https://www.cnblogs.com/wj-1314/p/9901029.html)
1. 博客：[端到端不定长文字识别 CRNN 算法详解](https://www.cnblogs.com/skyfsm/p/10335717.html)
1. 博客：[](https://www.cnblogs.com/skyfsm/p/10345305.html)[端到端不定长文本识别CRNN代码实现](https://www.cnblogs.com/skyfsm/p/10345305.html)
1. 博客：[长短期记忆网络（LSTM）](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
1. 博客：[语音识别：深入理解 CTC Loss 原理](https://blog.csdn.net/Left_Think/article/details/76370453)
1. 博客：[语音识别（LSTM+CTC）](https://www.cnblogs.com/followees/p/10422809.html)

<a name="oxtTX"></a>
### 关于我们
**Mo**（网址：[**momodel.cn**](https://momodel.cn/)）是一个支持 Python 的**人工智能在线建模平台**，能帮助你快速开发、训练并部署模型。

---

**Mo 人工智能俱乐部** 是由网站的研发与产品设计团队发起、致力于降低人工智能开发与使用门槛的俱乐部。团队具备大数据处理分析、可视化与数据建模经验，已承担多领域智能项目，具备从底层到前端的全线设计开发能力。主要研究方向为大数据管理分析与人工智能技术，并以此来促进数据驱动的科学研究。

目前俱乐部每周六在杭州举办以机器学习为主题的线下技术沙龙活动，不定期进行论文分享与学术交流。希望能汇聚来自各行各业对人工智能感兴趣的朋友，不断交流共同成长，推动人工智能民主化、应用普及化。

<br />

 <div>
    <h3>微信公众号: MomodelAI</h3>
    <img src='https://mo-imgs.momodel.cn/WeChatPublicAccount.png' height="220"/>
 </div>
 <br/>
 <br/>

<div>
    <h3>添加管理员微信加入AI俱乐部群聊</h3>
    <img src='https://mo-imgs.momodel.cn/wechat_office_contact.png' height="210">
</div>
